1. 아나콘다 가상환경 구축
    - Anaconda nevigator 가동
    - Environments > create > DataCrawling > python 3.6 > 생성 -> 업데이트 창 close
    - cd ~/analysis/crawling/requirements.txt 생성 (프로젝트 폴더 애널리스트->크롤링 밑에 txt생성)
    - requirements : 해당 프로젝트에서 사용하는 패키지를 기술함
      ---------------------------------------------------------
      base(root)에서의 requests=2.18.4, beautifulsoup4==4.6.0, pandas==0.23.0, numpy==1.14.3
      -> requirements.txt에 작성
      ---------------------------------------------------------
    - 데이터크롤링 환경에서 홈에서 쥬피터 install -> launch시키면 구동됨(2가지 방법)
      1.방법 : encvironments에서 구동 -> 커맨드창 생성됨(쥬피터 종료시 커맨드 끄고 꺼야지 완전 종료)
      2.방법 : 홈에서 launch 시키는 방법 
    - Environments > create > DataCrawling > open the terminal
        $ cd 프로젝트위치(requirements.txt 위치)
        -가상환경내에 설치된 패키지 목록 보기
        $ conda list or pip list : 단, 약간의 목록이 차이는 존재한다
        -프로젝트에 필요한 패키지 설치
        (DataCrawling)$ pip install -r requirements.txt   # 일단 시스템이서는 이거 쓴다 (파이썬환경)
        or 
        (DataCrawling)$ conda install --file requirements.txt  (실행)
        -># ('가상환경명') 
    ------------------------------------여기까지가 가상환경 설정 구간-----------------------------    
2. 주피터 구동

3. 데이터
   - 현쟁상황
   - 데이터의 종류
   - 데이터과학 진행과정 -> 이러한 절차적인 진행을 구상을 해야한다 2),3)이 제일 오래걸린다 60~70%에 해당
     1) 연구목표 설정 ( 이러한 이론들에 대한 서적들은 해외에 많다, 국내에는 그다지 없음 )
        - 무엇을 조사하고 싶은가? 그 분석 결과로 회사/공공에서는 어떤 이익을 도출할 것인가?
        - 어떤 데이터가 자원이 필요한가? 일정은 어떻게 되나? 업무분장은?
        - 가장 중요한 것은 최종 결과물에 대한 검토!! <- 시작점이 된다(엔딩에서의 의미가 시작점이 된다)
        - 이런 결과물은 의사결정에 재료가 될 수도, 업무의 한 파트가 될 수도 있다
        - 취직시에 어떤 것을 중점을 두고 프로젝트하는 회사에 가고싶은지가 중요하다(하고싶은것)
     2) 데이터 획득
        2-1. level 1 (데이터 수집과정이 간단하다)
            - 제공이 된다
            - 사내 데이터, 공공 데이터, 대학 및 연구기관의 제공 데이터
            - 콘테스트 데이터(국내대회, 해외대회(캐글kaggle))
            - > 상업성이 없고, 정제된 데이터다( 누구나가 다 볼수 있는 데이터다, 희소성없음 )
        2-2. level 2
            - open API 사용
            - http 통신을 통해서 응답 데이터를 통해 수집한다
            - ex) kakao, naver, t, Google 등등 포털이나 대기업이 제공하는 open API를 활용한다
            - 단, 쿼리제한(일일 쿼리수) -> 다양한 데이터 얻기가 힘들다
            - 정제된 데이터다(날것이 아님)
        2-3. level 3
            - web scraping( 웹 스크래핑 )
            - 우리가 접근할 수 있는 모든 정보는 웹에서 접근이 가능하다라는 명제로 출발
            - 보안 데이터는 접근이 불가하다
            - 웹사이트를 긁어서 원하는 데이터를 추출하여 전처리 적제하는 방식이다.
            - 여기서 사용하는 모듈은: request + beautifulsoup(bs4)
        2-4. level 4
            - crawling(크롤링) -> 이쪽 행위를 불법적으로 보는 시선이있다(프록시를 좀 알아야함)
            - 정보의 출처가 웹사이트가 맞는데 사람의 손을 타야지만 데이터를 획득할 수 있는 경우
            - ajax를 사용하거나, 디도스 방어가 들어갔거나, 등등 사람손을 거친후에야만 접근가능한 
              사이트가 대상
            - selenium(셀레니움) + 자동화(qt5 or 스케쥴러를 활용) : 이쪽 시장은 크다(음지가 많음)
     3) 데이터 준비
        - 이때 쓰는 패키지 : pandas(주로 사용, 데이터처리, 분석), numpy(수학, 과학용) 등을 사용
        - 데이터의 품질을 향상시킨다. 여기에 주안점이 있다
        < pandas를 주로 사용하는 용도 >
        3-1. 데이터 정제 : 결측치, 이상치 처리
        3-2. 데이터 통합 : 여러 군데에서 가져온 데이터를 종합해서 데이터를 구성한다
        3-3. 데이터 변환 : 데이터를 모델(4,5단계)에서 적합하게 사용되도록 변경 처리
     4) 데이터 탐색
        - 쓰이는 패키지 : pandas, matplotlib(시각화), seabon, d3.js(d3:시각화(검색해볼것))
          - ex) 유튜브에서 '데이터다람쥐'로 한번 볼것
          - d3는 딱 과거까지의 데이터를 시각화한다 (4단계까지의 프로세스)
        - 데이터의 깊이를 이해하는데 중점 -> 통찰
        - 변수들의 상호 작용성, 시각적 분석법, 이상점 존재여부 체크
        - 통계적 분석, 시각적 분석, 단순 모델링등을 사용(4단계까지의 수행 프로세스 공정)
----------------------------여기까지가 8월말까지의 교육 커리큘럼---------------------------------        
     5) 데이터 모델링 및 모델 구축 (데이터사이언티스트로서 갖는 공정) 
        - 쓰이는 패키지(미래예측에 관한 관점) : scikit-learn(머신러닝), tensorflow(딥러닝), keras(딥러닝)
          - 데이터 사이언티스트는 4단계에서 6단계로 건너뜀
        - 이전단계로부터 획득한 모델, 도메인 지식, 데이터에 대한 통찰등을 이용하여 연구 목표에 대한 답을 찾는 과정
          - 머신러닝, 딥러닝 : 예측모델을 구성, 정확도 높이고, 평가 지수 고려 등등 과정진행.
            => 목표치에 도달하지 못하면 다시 원점으로 돌아가서 다시 시작한다.( 전면 재검토(다시 2단계) )
        - 필요하다면 통계학도 사용, 머신러닝, 딥러닝 등등 운영과학기법들을 총동원한다
        - 머신러닝[학습, 정확도 평가, 파이프라인, 하이퍼파라미터 튜닝, 성능평가]    
     6) 시스템 통합 혹은 레포트 발표
        - 1단계에 정한 결론에 대한 마무리
        - 레포트 형태, 보고서 형태, 시스템 형태, 솔루션의 형태 등 다양한 모습으로 결론이 도출된다
          - 시스템 형태( 웹기반: flask, DJango / GUI:gt5 / 백그라운드 서비스:순수파이썬형태+OS종속적구조)
   - 데이터 획득

#naver API 사용을 위한 키,
#dev.naver.com -> 어플리케이션 등록 -> 아이피 등록 -> 아이디/암호 받을것.( 네이버에서 내 아이피 검색후 http//붙여넣기)
#https://developers.naver.com/docs/datalab/search/ 다큐먼트->서비스api->데이터랩->통합검색어트랜드->파이썬
# 데이터 추출은 서비스api -> 검색 -> 카데고리 선택 